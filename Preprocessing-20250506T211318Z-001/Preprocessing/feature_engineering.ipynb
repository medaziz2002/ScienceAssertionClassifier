{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#0- Imports"],"metadata":{"id":"AYi622GULVYS"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","import re\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","from collections import Counter\n","import nltk\n","import string"],"metadata":{"id":"92oim1uP8Ho_","executionInfo":{"status":"ok","timestamp":1745334752236,"user_tz":-120,"elapsed":1450,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Download necessary NLTK resources if not already downloaded\n","\n","try:\n","    nltk.data.find('sentiment/vader_lexicon')\n","except LookupError:\n","    nltk.download('vader_lexicon')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgimcfT8DCEJ","executionInfo":{"status":"ok","timestamp":1745334753450,"user_tz":-120,"elapsed":103,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}},"outputId":"21a47016-fbc7-4995-ca42-3886205a000f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]}]},{"cell_type":"markdown","source":["#1- counts"],"metadata":{"id":"ous35vuX8JSI"}},{"cell_type":"code","source":["def count_mentions(text):\n","    \"\"\"Count @mentions in text.\"\"\"\n","    mention_pattern = re.compile(r'@\\w+')\n","    return len(mention_pattern.findall(text))\n","\n","def count_hashtags(text):\n","    \"\"\"Count #hashtags in text.\"\"\"\n","    hashtag_pattern = re.compile(r'#\\w+')\n","    return len(hashtag_pattern.findall(text))\n","\n","def count_punctuation(text):\n","    \"\"\"Count punctuation marks in text.\"\"\"\n","    return sum([1 for char in text if char in string.punctuation])\n","\n","def count_capital_letters(text):\n","    \"\"\"Count capital letters in text.\"\"\"\n","    return sum(1 for c in text if c.isupper())\n","\n","def text_length(text):\n","    \"\"\"Calculate text length.\"\"\"\n","    return len(text)\n","\n","def word_count(text):\n","    \"\"\"Calculate word count.\"\"\"\n","    return len(text.split())\n","\n","def average_word_length(text):\n","    \"\"\"Calculate average word length.\"\"\"\n","    words = text.split()\n","    if not words:\n","        return 0\n","    return sum(len(word) for word in words) / len(words)"],"metadata":{"id":"LZh8Uu_88MZ8","executionInfo":{"status":"ok","timestamp":1745334863538,"user_tz":-120,"elapsed":4,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#2- Sentiment analysis"],"metadata":{"id":"VPCBtrz98Ywv"}},{"cell_type":"code","source":["def sentiment_analysis(text):\n","    \"\"\"Perform sentiment analysis on text.\"\"\"\n","    sia = SentimentIntensityAnalyzer()\n","    sentiment = sia.polarity_scores(text)\n","    return sentiment"],"metadata":{"id":"r-T5796r8hNS","executionInfo":{"status":"ok","timestamp":1745334894046,"user_tz":-120,"elapsed":3,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#3- hases"],"metadata":{"id":"04HpHeG78k1Y"}},{"cell_type":"code","source":["def has_scientific_keywords(text):\n","    \"\"\"Check if text contains scientific keywords.\"\"\"\n","    scientific_keywords = ['study', 'research', 'science', 'scientist', 'evidence', 'experiment',\n","                          'data', 'analysis', 'hypothesis', 'theory', 'clinical', 'medical',\n","                          'findings', 'journal', 'publication', 'published', 'effects']\n","\n","    return any(keyword in text.lower() for keyword in scientific_keywords)\n","\n","def has_question(text):\n","    \"\"\"Check if text contains a question.\"\"\"\n","    return '?' in text\n","\n","def has_numbers(text):\n","    \"\"\"Check if text contains numbers.\"\"\"\n","    return bool(re.search(r'\\d', text))"],"metadata":{"id":"d6QF3CTd8n6Q","executionInfo":{"status":"ok","timestamp":1745334920368,"user_tz":-120,"elapsed":3,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#4- create features"],"metadata":{"id":"x8b6wI_m8rjU"}},{"cell_type":"code","source":["def create_bow_features(texts, max_features=1000):\n","    \"\"\"Create bag of words features.\"\"\"\n","    vectorizer = CountVectorizer(max_features=max_features)\n","    bow_features = vectorizer.fit_transform(texts)\n","    return bow_features, vectorizer\n","\n","def create_tfidf_features(texts, max_features=1000):\n","    \"\"\"Create TF-IDF features.\"\"\"\n","    vectorizer = TfidfVectorizer(max_features=max_features)\n","    tfidf_features = vectorizer.fit_transform(texts)\n","    return tfidf_features, vectorizer\n"],"metadata":{"id":"pgEL4cig8vig","executionInfo":{"status":"ok","timestamp":1745334951412,"user_tz":-120,"elapsed":8,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["#5- extract basic features"],"metadata":{"id":"ffWYyGqC8yII"}},{"cell_type":"code","source":["def extract_basic_features(df, text_column='text'):\n","    \"\"\"Extract basic features from text data.\"\"\"\n","    # Create a copy to avoid modifying the original\n","    feature_df = df.copy()\n","\n","    # Extract basic features\n","    feature_df['text_length'] = feature_df[text_column].apply(text_length)\n","    feature_df['word_count'] = feature_df[text_column].apply(word_count)\n","    feature_df['avg_word_length'] = feature_df[text_column].apply(average_word_length)\n","    #feature_df['url_count'] = feature_df[text_column].apply(count_urls)\n","    feature_df['mention_count'] = feature_df[text_column].apply(count_mentions)\n","    feature_df['hashtag_count'] = feature_df[text_column].apply(count_hashtags)\n","    feature_df['punctuation_count'] = feature_df[text_column].apply(count_punctuation)\n","    feature_df['capital_letter_count'] = feature_df[text_column].apply(count_capital_letters)\n","    feature_df['has_question'] = feature_df[text_column].apply(has_question)\n","    feature_df['has_numbers'] = feature_df[text_column].apply(has_numbers)\n","    feature_df['has_scientific_keywords'] = feature_df[text_column].apply(has_scientific_keywords)\n","\n","    # Extract sentiment features\n","    sentiments = feature_df[text_column].apply(sentiment_analysis)\n","    feature_df['sentiment_negative'] = sentiments.apply(lambda x: x['neg'])\n","    feature_df['sentiment_neutral'] = sentiments.apply(lambda x: x['neu'])\n","    feature_df['sentiment_positive'] = sentiments.apply(lambda x: x['pos'])\n","    feature_df['sentiment_compound'] = sentiments.apply(lambda x: x['compound'])\n","\n","    return feature_df"],"metadata":{"id":"OSQame4t83NZ","executionInfo":{"status":"ok","timestamp":1745335357228,"user_tz":-120,"elapsed":3,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["#6- feature engineering"],"metadata":{"id":"NY0DL2GH84kU"}},{"cell_type":"code","source":["def feature_engineering_pipeline(df, text_column='text', max_features=1000):\n","    \"\"\"Complete feature engineering pipeline.\"\"\"\n","    # Extract basic features\n","    feature_df = extract_basic_features(df, text_column)\n","\n","    # Create TF-IDF features\n","    tfidf_features, tfidf_vectorizer = create_tfidf_features(feature_df[text_column], max_features)\n","\n","    # Convert sparse matrix to DataFrame\n","    tfidf_df = pd.DataFrame(\n","        tfidf_features.toarray(),\n","        columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])],\n","        index=feature_df.index\n","    )\n","\n","    # Combine all features\n","    final_df = pd.concat([feature_df, tfidf_df], axis=1)\n","\n","    return final_df, tfidf_vectorizer"],"metadata":{"id":"rnWlBBHE8-fZ","executionInfo":{"status":"ok","timestamp":1745335002299,"user_tz":-120,"elapsed":16,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["#7- main"],"metadata":{"id":"OK1Pojaq8_j1"}},{"cell_type":"code","source":["# Test functions with outputs\n","if __name__ == \"__main__\":\n","    # Load the actual dataset\n","    print(\"Loading dataset...\")\n","    data = pd.read_csv('scitweets_export.tsv', sep='\\t')\n","\n","    print(\"Dataset information:\")\n","    print(f\"Shape: {data.shape}\")\n","    print(\"\\nColumns:\")\n","    print(data.columns.tolist())\n","    print(\"\\nSample data:\")\n","    print(data.head(2))\n","\n","    # Sample a small subset for testing to avoid memory issues\n","    sample_size = min(1000, len(data))\n","    print(f\"\\nUsing a sample of {sample_size} records for testing...\")\n","    sample_data = data.sample(sample_size, random_state=42)\n","\n","    # Test individual feature extraction functions on a single example\n","    if not sample_data.empty:\n","        text = sample_data['text'].iloc[0]\n","        print(\"\\nSample text for feature extraction:\")\n","        print(text)\n","\n","        print(f\"\\nText length: {text_length(text)}\")\n","        print(f\"Word count: {word_count(text)}\")\n","        print(f\"Average word length: {average_word_length(text)}\")\n","\n","        print(f\"Mention count: {count_mentions(text)}\")\n","        print(f\"Hashtag count: {count_hashtags(text)}\")\n","        print(f\"Punctuation count: {count_punctuation(text)}\")\n","        print(f\"Capital letter count: {count_capital_letters(text)}\")\n","        print(f\"Has question: {has_question(text)}\")\n","        print(f\"Has numbers: {has_numbers(text)}\")\n","        print(f\"Has scientific keywords: {has_scientific_keywords(text)}\")\n","\n","        print(\"\\nSentiment analysis:\")\n","        sentiment = sentiment_analysis(text)\n","        print(sentiment)\n","\n","    # Extract basic features\n","    print(\"\\nExtracting basic features...\")\n","    basic_features_df = extract_basic_features(sample_data)\n","    print(\"\\nBasic features statistics:\")\n","    print(basic_features_df.describe())\n","\n","    # Test BoW features with a reasonable number of features\n","    print(\"\\nCreating Bag of Words features...\")\n","    bow_features, bow_vectorizer = create_bow_features(sample_data['text'], max_features=100)\n","    print(f\"BoW shape: {bow_features.shape}\")\n","    print(f\"Top 10 BoW feature names: {bow_vectorizer.get_feature_names_out()[:10]}...\")\n","\n","    # Test TF-IDF features\n","    print(\"\\nCreating TF-IDF features...\")\n","    tfidf_features, tfidf_vectorizer = create_tfidf_features(sample_data['text'], max_features=100)\n","    print(f\"TF-IDF shape: {tfidf_features.shape}\")\n","    print(f\"Top 10 TF-IDF feature names: {tfidf_vectorizer.get_feature_names_out()[:10]}...\")\n","\n","    # Run complete feature engineering pipeline\n","    print(\"\\nRunning complete feature engineering pipeline...\")\n","    try:\n","        final_df, _ = feature_engineering_pipeline(sample_data, max_features=100)\n","        print(f\"Final DataFrame shape: {final_df.shape}\")\n","        print(\"\\nFeature categories in final DataFrame:\")\n","        # Group columns by type for better readability\n","        basic_cols = [col for col in final_df.columns if not col.startswith('tfidf_') and col not in ['tweet_id', 'text']]\n","        tfidf_cols = [col for col in final_df.columns if col.startswith('tfidf_')]\n","        original_cols = [col for col in final_df.columns if col in data.columns and col not in ['tweet_id', 'text']]\n","\n","        print(f\"- Original data columns: {len(original_cols)}\")\n","        print(f\"- Basic text features: {len(basic_cols)}\")\n","        print(f\"- TF-IDF features: {len(tfidf_cols)}\")\n","\n","        # Display summary statistics for some key features\n","        print(\"\\nSummary statistics for key features:\")\n","        key_features = ['text_length', 'word_count', 'sentiment_compound', 'sentiment_positive', 'sentiment_negative']\n","        print(final_df[key_features].describe())\n","\n","        # Check correlation with target variable if it exists in the dataset\n","        target_columns = [col for col in final_df.columns if col in ['science_related', 'scientific_claim', 'scientific_reference', 'scientific_context']]\n","        if target_columns:\n","            print(\"\\nCorrelation with target variables:\")\n","            for target in target_columns:\n","                print(f\"\\nTop 10 features correlated with {target}:\")\n","                correlations = final_df.corr()[target].sort_values(ascending=False)\n","                print(correlations.head(11))  # 11 to include the target itself\n","\n","    except Exception as e:\n","        print(f\"Error in feature engineering pipeline: {e}\")"],"metadata":{"id":"rIuV00FT9ELS","executionInfo":{"status":"ok","timestamp":1745335378968,"user_tz":-120,"elapsed":17071,"user":{"displayName":"Imene Bouaziz","userId":"18205929105057008466"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"56e516d8-1129-496e-d774-0d5fd4278a53"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Dataset information:\n","Shape: (1140, 7)\n","\n","Columns:\n","['Unnamed: 0', 'tweet_id', 'text', 'science_related', 'scientific_claim', 'scientific_reference', 'scientific_context']\n","\n","Sample data:\n","   Unnamed: 0            tweet_id  \\\n","0           0  316669998137483264   \n","1           1  319090866545385472   \n","\n","                                                text  science_related  \\\n","0  Knees are a bit sore. i guess that's a sign th...                0   \n","1          McDonald's breakfast stop then the gym üèÄüí™                0   \n","\n","   scientific_claim  scientific_reference  scientific_context  \n","0               0.0                   0.0                 0.0  \n","1               0.0                   0.0                 0.0  \n","\n","Using a sample of 1000 records for testing...\n","\n","Sample text for feature extraction:\n","It‚Äôs phony Christian Sunday. People who support a hateful administration & then attend church are phonies...PERIOD\n","\n","Text length: 114\n","Word count: 16\n","Average word length: 6.1875\n","Mention count: 0\n","Hashtag count: 0\n","Punctuation count: 5\n","Capital letter count: 10\n","Has question: False\n","Has numbers: False\n","Has scientific keywords: False\n","\n","Sentiment analysis:\n","{'neg': 0.179, 'neu': 0.67, 'pos': 0.151, 'compound': -0.128}\n","\n","Extracting basic features...\n","\n","Basic features statistics:\n","        Unnamed: 0      tweet_id  science_related  scientific_claim  \\\n","count  1000.000000  1.000000e+03      1000.000000        1000.00000   \n","mean    622.963000  8.537032e+17         0.330000           0.23200   \n","std     364.789291  2.868099e+17         0.470448           0.42232   \n","min       0.000000  3.166700e+17         0.000000           0.00000   \n","25%     304.750000  6.168526e+17         0.000000           0.00000   \n","50%     617.500000  8.563339e+17         0.000000           0.00000   \n","75%     939.250000  1.103035e+18         1.000000           0.00000   \n","max    1260.000000  1.344485e+18         1.000000           1.00000   \n","\n","       scientific_reference  scientific_context  text_length   word_count  \\\n","count           1000.000000         1000.000000  1000.000000  1000.000000   \n","mean               0.179000            0.219000   137.245000    19.268000   \n","std                0.383544            0.413776    70.670031    10.452245   \n","min                0.000000            0.000000    14.000000     2.000000   \n","25%                0.000000            0.000000    93.000000    12.000000   \n","50%                0.000000            0.000000   125.000000    17.000000   \n","75%                0.000000            0.000000   156.000000    23.000000   \n","max                1.000000            1.000000   777.000000    66.000000   \n","\n","       avg_word_length  mention_count  hashtag_count  punctuation_count  \\\n","count      1000.000000    1000.000000    1000.000000        1000.000000   \n","mean          6.451479       0.682000       0.648000           8.519000   \n","std           1.625318       2.234249       1.365269           5.559726   \n","min           3.000000       0.000000       0.000000           0.000000   \n","25%           5.294402       0.000000       0.000000           5.000000   \n","50%           6.264912       0.000000       0.000000           8.000000   \n","75%           7.295588       1.000000       1.000000          11.000000   \n","max          15.500000      50.000000      17.000000          67.000000   \n","\n","       capital_letter_count  sentiment_negative  sentiment_neutral  \\\n","count           1000.000000         1000.000000        1000.000000   \n","mean              11.947000            0.107141           0.766333   \n","std               11.687977            0.140411           0.165324   \n","min                0.000000            0.000000           0.208000   \n","25%                5.000000            0.000000           0.654000   \n","50%               10.000000            0.000000           0.777000   \n","75%               15.000000            0.186250           0.882000   \n","max              104.000000            0.748000           1.000000   \n","\n","       sentiment_positive  sentiment_compound  \n","count         1000.000000         1000.000000  \n","mean             0.126525            0.065040  \n","std              0.133668            0.495135  \n","min              0.000000           -0.964800  \n","25%              0.000000           -0.296000  \n","50%              0.108000            0.000000  \n","75%              0.205000            0.442275  \n","max              0.670000            0.983800  \n","\n","Creating Bag of Words features...\n","BoW shape: (1000, 100)\n","Top 10 BoW feature names: ['about' 'after' 'all' 'an' 'and' 'are' 'as' 'at' 'be' 'being']...\n","\n","Creating TF-IDF features...\n","TF-IDF shape: (1000, 100)\n","Top 10 TF-IDF feature names: ['about' 'after' 'all' 'an' 'and' 'are' 'as' 'at' 'be' 'being']...\n","\n","Running complete feature engineering pipeline...\n","Final DataFrame shape: (1000, 121)\n","\n","Feature categories in final DataFrame:\n","- Original data columns: 5\n","- Basic text features: 19\n","- TF-IDF features: 100\n","\n","Summary statistics for key features:\n","       text_length   word_count  sentiment_compound  sentiment_positive  \\\n","count  1000.000000  1000.000000         1000.000000         1000.000000   \n","mean    137.245000    19.268000            0.065040            0.126525   \n","std      70.670031    10.452245            0.495135            0.133668   \n","min      14.000000     2.000000           -0.964800            0.000000   \n","25%      93.000000    12.000000           -0.296000            0.000000   \n","50%     125.000000    17.000000            0.000000            0.108000   \n","75%     156.000000    23.000000            0.442275            0.205000   \n","max     777.000000    66.000000            0.983800            0.670000   \n","\n","       sentiment_negative  \n","count         1000.000000  \n","mean             0.107141  \n","std              0.140411  \n","min              0.000000  \n","25%              0.000000  \n","50%              0.000000  \n","75%              0.186250  \n","max              0.748000  \n","\n","Correlation with target variables:\n","\n","Top 10 features correlated with science_related:\n","Error in feature engineering pipeline: could not convert string to float: 'It‚Äôs phony Christian Sunday. People who support a hateful administration & then attend church are phonies...PERIOD'\n"]}]}]}